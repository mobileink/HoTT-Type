\documentclass{article}

\usepackage{hottmacros}
%% \usepackage{prftree.sty}

\usepackage{bussproofs}

\usepackage{fontspec,xltxtra,xunicode}
\usepackage{fontspec}
\defaultfontfeatures{Scale=MatchLowercase}

%% \setmainfont[Mapping=tex-text]{Times New Roman}
%% \setromanfont[Mapping=tex-text]{Times New Roman}
%% \setsansfont[Mapping=tex-text]{Arial}

\setmainfont[Mapping=tex-text]{TeX Gyre Pagella}
%% \setromanfont[Mapping=tex-text]{TeX Gyre Pagella}
%% \setsansfont[Mapping=tex-text]{TeX Gyre Heros}

\usepackage{tikz}

\usetikzlibrary{arrows,shapes,patterns,backgrounds,spy}
\usepackage{pgffor}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\title{Inference}
\maketitle
\large

\tableofcontents

\section{Preliminaries}

\subsection{Induction}

http://planetmath.org/wellfoundedinduction

\section{Varieties of Inference}

\begin{itemize}
\item \(a{:}A\) \quad Type inference: from token \(a\) infer type \(A\)

  Language entry move?
\item \(A\vdash B\) \quad Warrant:  \(A\) warrants inference to \(B\) (indirect proof)

  This is a sequent; \(A\) is a structure, \(B\) is a formula (proposition).
\item \(X\models Y\) \quad Proof:  \(X\) proves \(Y\)

  Here \(X\) and \(Y\) must be sequents?
\end{itemize}

\begin{remark}
  If we treat : as an inference op, then we must treat a and A as
  propositions.  So we can read a:A two ways: a proves A
  (proposition), and a entails A (inference).  But treating a:A as a
  proposition won't work because then we would have (a:A):U.  But
  treating it as an inference means we need a way to construe a as a
  proposition.  No stranger than treating A as a proposition in the
  end.  If we go with type instead of proposition, we treat a as a
  type, which is maybe ok if we mean by that the ``natural type'' of
  tokens a.  Either way, the idea that a:A is a ``judgment'' is
  unintelligible.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Logical Constants}

At least, at most.

We can treat \(\lor\) as ``at least''.

Restall p. 34:

\begin{prooftree}
  \AxiomC{\(Y(A)\vdash C\)}
  \AxiomC{\(Y(B)\vdash C\)}
  \AxiomC{\(X\vdash A\lor B\)}
  \RightLabel{\(\lor\)-\textsc{e}}
  \TrinaryInfC{\(Y(X)\vdash C\)}
\end{prooftree}

Restall, p. 106:

\begin{prooftree}
  \AxiomC{\(X(A)\vdash C\)}
  \AxiomC{|}
  \AxiomC{\(X(B)\vdash C\)}
  \RightLabel{\(\lor\)-\textsc{l}}
  \TrinaryInfC{\(X(A\lor B)\vdash C\)}
\end{prooftree}

\subsection{Construction and Coconstruction}

Construction: start from nothing and build up.

Coconstruction: start from something and build down?

Every type has constructors (intro rules) and coconstructors (elim rules).

For infinite streams: a constructor will look like (n, next), where
next is a function such that next(n) \(\succ\) n.  Then head(n,next) =
n, and tail(n,next) = (next(n), next).

\begin{remark}
FIXME: this is wrong.  For nat streams, we
have \[\textsf{next}(\langle 2,3,\ldots\rangle) = \langle
3,\ldots\rangle\] so by set inclusion: \(\langle
2,3\ldots\rangle\succ\textsf{next}(\langle 2,3,\ldots\rangle)\).
Which is exactly what we have for next on n:Nat when recurring down.
\end{remark}

\begin{remark}
  Terminology: we have ``initial segment'', and if \(n\le m\) then the
  initial segment at \(n\) is contained in (precedes) the initial
  segment at \(m\).  The stream starting at \(n\) we will call the
  ``co-initial segment at \(n\)'', and if \(n\le m\) then the
  co-initial segment at \(n\) contains (succeeds) the co-initial
  segment at \(m\), so the ordering is reversed.  In other words,
  initial and co-initial segments are dual.
\end{remark}

For finite objects, e.g. naturals, we can do the same thing using a
decreasing next, such that next(n) \(\prec\) n.  In this case, head(n,
next) = n and tail(n, next) = (next(n), next) = (n-1, next).  (Of
course we have to deal with the base case but that's a detail.)

Thus the types Nat and Stream Nat are structurally the same for elim,
the difference is in the meaning of the ``next'' function.  Which
makes them dual?

For construction, same duality.  For any n:Nat, we have S(n):Nat, with
\(n\prec S(n)\); for any n:Stream Nat, we have tail(n):Stream Nat,
with \(n\succ tail(n)\).  So tail is to Streams as S is for Nats.  In
the latter case, we call S a constructor, in the former, we call tail
a co-constructor; in both cases, we build something ``new'' from
something we already have.

Co-construction is sometimes construed as ``observation'', or better,
experimentation: apply the ``stimulus'' tail to the sample s:Stream
Nat and observe the result: tail(s):Stream Nat.  Of course we could
also construe construction in the same way: treat S as the stimulus,
apply it to a sample n:Nat, and observe the result: S(n):Nat.  In
other words, the distinction between construction and observation that
is often cited in writings on coinduction and bisimulation is not very
revealing: construction can be construed as experimentation
(stimulate-and-observe), and so can coconstruction.  It's really a
matter of perspective.  Or perhaps the key distinction is where you
start out and where you end up: with construction we start from
nothing and build up indefinitely from a base case, or we start from
something and build down to base case; with coconstruction we start
with nothing and cobuild once from any seed (e.g (0,+1):Stream Nat),
or we start from something and cobuild up indefinitely using the
constructive method of the base case.

Which implies that cobuilding starts with infinitely many base cases
(``seeds'' and ``next'' functions), in contrast to building, which
starts with finitely many base cases. (is this true?)  Which would
make building a special case of cobuilding, with constant seeds and
next functions.

In any case, there are three basic aspects to induction/coinduction:

\begin{itemize}
\item Where you start out - ``base'' cases (finite v. infinite?)
\item Where you end up - convergence (terminating proc) or divergence
  (non-terminating proc)
\item How you proceed - advancing (next(n) \(\succ\) n) or receding
  (next(n) \(\prec\) n)
\end{itemize}

Different configurations of these three elements give different
``principles''.  Induction: finite base cases, terminating proc,
advancing constructor, receding recursor.  Coinduction: infinite base
cases (?), non-terminating proc(?), etc.???

\begin{remark}
  Is tail for streams advancing or receding?  Do we have n\(\prec\)
  tail(n) for streams?  Shouldn't it be the other way around?  The
  stream at 2 contains the stream at 3, so it should be greater even
  though it ``comes first''.  We have \(2\prec 3\), but (using
  \(\langle a,\ldots\rangle\) to indicate the stream at \(a\)):
  \(\langle 2,3,\ldots\rangle\succ\langle 3,\ldots\rangle\).
  Conversely, the initial segment at \(2\) is contained in the initial
  segment at \(3\).  Giving:
\end{remark}

\begin{itemize}
\item \(\langle 0,1,2\rangle \subset \langle 0,1,2,3\rangle \to\langle 0,1,2\rangle\prec\langle 0,1,2,3\rangle\)
\item \(\langle 2,3,\ldots\rangle\supset\langle 3,\ldots\rangle\to\langle 2,3,\ldots\rangle\succ\langle 3,\ldots\rangle\).
\end{itemize}

which implies that using tail on streams counts as ``building down'':
even though the result is always another infinite object, next(s) is
``less than'' s, since it is contained in it.  In other words, order
is not based on cardinality but on ordinality (set inclusion).  This
allows us to order infinities like streams.  (If \(\Nat\) qua ordinal
is \(\omega\), then can we use the latter as the type of streams of
Nats?)

\begin{remark}
  Does this really work?  Types involve both induction and
  coinduction; either may advance or recede, etc.
\end{remark}

Recursion and corecursion are different from induction and
coinduction.  Recursion uses decreasing ``tail'' and ends up at the
base case; corecursion uses increasing tail and never terminates.
That's the key difference, not construction v. observation.

This means that so-called infinite objects like streams are
constructive, they are implemented using a ``next'' function that
allows is to construct the head of the stream as we go along.

Just as Succ allows us to construct any n:Nat by running through any
initial segment, constructing each element as we go, Tail allows us to
(co-)construct the tail of any initial segment by running through
initial segments, removing each element as we go, thus co-constructing
as we go.

Infinite data structures are special cases of coinduction.  In other
words, coinduction works for finite as well as infinite data, just
like induction does.

\subsection{Introduction and Elimination rules as Translation}

\begin{remark}
  This really only works for sequent calculi.
\end{remark}

Example:

\AxiomC{\(A\enspace\textvisiblespace\enspace B\)}
\RightLabel{\(\land\)-\textsc{trans}\(\uparrow\)}
\UnaryInfC{\(A\land B\)}
\DisplayProof

\medskip
Here the \(\land\)-\textsc{trans} rule translates \textvisiblespace\ to
\(\land\).  Informally, it takes \(A\) and \(B\) separately (read
\(A\textvisiblespace B\) as ``\(A\) together with \(B\)'') and
combines them into a unity \(A\land B\).


\bigskip
\quad\quad
\AxiomC{\(A\land B\)}
\RightLabel{\(\land\)-\textsc{trans}\(\downarrow^1\)}
\UnaryInfC{\(A\)}
\DisplayProof
\quad
\AxiomC{\(A\land B\)}
\RightLabel{\(\land\)-\textsc{trans}\(\downarrow^2\)}
\UnaryInfC{\(B\)}
\DisplayProof

\medskip
It is less clear why we should treat elimination rules as involving
translation.  In each case, what is eliminated is not only \(\land\)
but also one of the conjuncts.  We can make this more explicit:

\bigskip
\quad\quad
\AxiomC{\(A\land B\)}
\RightLabel{\(\land\)-\textsc{trans}\(\downarrow^1\)}
\UnaryInfC{\(A\enspace\textvisiblespace\enspace \_\)}
\DisplayProof
\quad
\AxiomC{\(A\land B\)}
\RightLabel{\(\land\)-\textsc{trans}\(\downarrow^2\)}
\UnaryInfC{\(\_\enspace\textvisiblespace\enspace B\)}
\DisplayProof

\bigskip

In the case of \(\to\)-\textsc{intro}, we can treat \(\to\)
as the translation of the implicit inference from assumed \(A\) to
\(B\).  Introducing \(A\to B\) ``discharges'' the assumption \(A\),
but not the proof (inference) from that assumption to \(B\): the
proposition \(A\to B\) remains dependent on that proof.

In the case of \(\to\)-\textsc{elim} (\emph{modus ponens}), the
translational interpretation is even more obscure.  The conclusion is
simply \(B\):

\medskip
\AxiomC{\(A\to B\enspace\textvisiblespace\enspace A\)}
\RightLabel{\(\to\)-\textsc{trans}\(\downarrow\)}
\UnaryInfC{\(B\)}
\DisplayProof

\medskip
A possible reading, using [\ldots] to indicate implicit stuff:

\bigskip
\AxiomC{\(A\to B\enspace\textvisiblespace\enspace A\)}
\RightLabel{\(\to\)-\textsc{trans}\(\downarrow\)}
\UnaryInfC{\([\_;A\vdash] B\)}
\DisplayProof

\medskip
Here \(\to\) translates to \(\vdash\), and the pair of premises
translates to an implicit structure (indicated by \(\_;A\)).

Ok, that's a bit of a stretch.  Modus ponens is a little special:
implicitly, at least, it involves a kind of interaction between the
\(\to\) embedded in the major premise and the minor premise.  In other
words, the major premise is not opaque in the way that the premises of
say \(\land\)-\textsf{intro} are.

Maybe we should say that \(\to\) and \textvisiblespace\ anihilate each
other:

\bigskip
\AxiomC{\(A\enspace\textvisiblespace\enspace A\to B\)}
\RightLabel{\(\to\)-\textsc{trans}\(\downarrow\)}
\UnaryInfC{\([A\enspace\textvisiblespace\enspace A\to]\ B\)}
\DisplayProof

\subsection{Logical Constants as Inference Markers}

Frapolli and Assimakaopoulus claim (based on relevance logics) that
conjunction ``loses'' the conjuncts: ``propositions connected by a
conjunction stop being independent pieces of information and become a
single complex piece...the presence of `and' specifically excludes any
interpretation in which the two conjoined propositions can be said to
stand in any kind of relation.  Therefore, a fortiori, `and' cannot be
seen as marking any kind of transition between the conjuncts, let
alone a truth-preserving one.'' p. 636

Two problems with this.  One is that it is easy to see the conjoints
as standing in a relation of mutual material implication.  If we have
\(P\land Q\), then we have \(P\leftrightarrow Q\).  The other problem
is that there is no principled way to restrict this sort of
interpretation to conjunction only.  The same considerations ought to
apply to any binary operator, in particular to \(\to\).

F and A give the following as an example backing up their claim about
conjunction:

\begin{enumerate}
\item John fell.  He slipped on the we pavement.
\item John fell and he slipped on the we pavement.
\end{enumerate}

The argument is that in 1) the second sentence is ``naturally''
interpreted as an ``explanation'' of the first, but in 2) the second
conjunct cannot be interpreted in that way.  This is obviously a very
contestable interpretation of these sentences, but even if we let it
stand, we can do the same with implication.  Using one of Brandom's
favorite examples:

\begin{enumerate}
\item Pittsburgh is west of Princeton.  Princeton is east of Pittsburgh.
\item If Pittsburgh is west of Princeton, then Princeton is east of Pittsburgh.
\end{enumerate}

In 1), the second sentence is ``naturally'' interpreted as following
from the first, but in 2) it cannot be interpreted that way, since 2)
is a hypothetical in which both propositions are embedded and neither
is asserted.  Strictly speaking an implication does not contain a
premise and a conclusion; it is a proposition, not an inference, so
the second clause cannot be interpreted as a consequence of the first,
\emph{unless} the first happens to be true.  That only happens under
modus ponens.  So we can treat implication just as F and A treat
conjunction, as a''single complex piece'' of information in which the
embedded clauses cannot be separated so \(\to\) cannot be seen as a
link between them.

\begin{remark}
  Most people would probably say that the second clause follows from
  the first in the conditional sentence.  But that only works if you
  first separate those clauses and make them independent sentences.
  We might want to read the conditional as \emph{saying that} the
  second clause \emph{would} follow from the first \emph{if} they were
  both separated and the first asserted; but we would not want to read
  it as \emph{showing} that, or as \emph{saying} (asserting) either
  clause.  Therefore \(\to\) cannot be read as marking an inferential
  link between actual (asserted) sentences.  On the other hand it
  clearly seems to indicate a relation of some kind.  Maybe as marking
  a schematic inferential license between sentence \emph{types}?
\end{remark}

\begin{remark}
  Can we read conjunction as involving embedded sentences, and thus
  explainable as a (bi-)conditional?  The natural way to read \(P\land
  Q\) is probably to take it as asserting both \(P\) and \(Q\); but
  strictly speaking this is not the case: what it asserts is the
  \(\land\)-combination of \(P\) and \(Q\); it does not assert the
  conjuncts separately.  That's what \(\land\)-elimination is for.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Consequence, Implication, Inference}

Logical v. material.

\(P\models Q\) :  logical consequence.  Q follows P of logical necessity.

\(P\to Q\) : material implication.  Q follows P of material (local) necessity.

Material implication is so-called because it depends on particular
interpretations?  You must derive material implication by assuming P
and then proving Q, so the implication depends in each case on what P
and Q mean.  Logical consequence is not derived, at least not
model-theoretic logical consequence, and it does not matter what P and
Q are.  In other words we do not infer logical consequence.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof}

Proof subsumes warrant:  \(X\models Y\) implies \(X\vdash Y\), but not vice-versa.

Direct proof subsumes indirect proof; or, a direct proof is always
also an indirect proof.  Which means that it is always possible to
dilute a direct proof by inserting unneccessary steps, but it is not
always possible to reduce an indirect proof to a direct proof.

Canonical proofs always end in a direct proof; that is, the last step
must be canonical.

In other words we need to make a distinction between the direct proofs
of one step - these are just the instances of canonical intro rules -
and canonical proofs that may have many steps but end in a canonical
step.  Or something like that.

Canonical inferences (i.e. intro rules) have a dual character;
\(P;Q\models P\land Q\) implies \(P;Q\vdash P\land Q\), but also the
other way around.  This is because the inference rule \emph{defines}
\(P\land Q\).

\begin{remark}
  We need to make a distinction here between natural deduction and the
  sequent calculus, and make sure the mapping between horizontal and
  vertical presentations is clear.
\end{remark}

\subsection{The Subformula Principle}


\section{Classical Logic}

\subsection{Truth Tables}

\section{Natural Deduction}


\begin{tikzpicture}[
    edge from parent path={
        (\tikzparentnode\tikzparentanchor)
        +(0pt,.5\tikzleveldistance)
        -- (0pt,-.5\tikzleveldistance -| \tikzchildnode\tikzchildanchor)
        -- +(0.25cm,0pt)
        -- +(-0.25cm,0pt)
    },
    grow'=up,
    level distance=4ex,
    level/.style={sibling distance=3em/#1}]
  \node (Concl) {P\land Q}
    child { node (Major) {P} }
    child { node (Minor) {Q} } ;

\end{tikzpicture}

\bigskip

\begin{tikzpicture}[
    edge from parent path={
        (\tikzparentnode\tikzparentanchor)
        +(0pt,.5\tikzleveldistance)
        -- (0pt,-.5\tikzleveldistance -| \tikzchildnode\tikzchildanchor)
        -- +(0.25cm,0pt)
        -- +(-0.25cm,0pt)
    },
    grow'=up,level distance=4ex,
    level/.style={sibling distance=10em/#1}]
  \node (A) {A}
    child { node (B) {B}
      child { node (D) {D} }
      child { node (E) {E} } }
    child { node (C) {C}
      child { node (F) {F} }
      child { node (G) {G}
              child { node (H) {H} }
              child { node (I) {I} }
              child { node (J) {J} } } } ;

\end{tikzpicture}


\section{Sequent Calculus}

The conclusion of a proof is a consecution, of the form \(X\vdash Y\).
What this means, in effect, is that by virtue of the subformula
property, the \(\vdash\) in the conclusion serves as a kind of
syntactic sugar for the fully expanded proof.

In other words, we can think of ``prove \(X\vdash Y\)'' as an
instruction to expand the \(\vdash\) into a complete proof tree.

\subsection{Substructural Logics}

\begin{remark}
  Can we do substructural logics without sequents?  Presumably yes,
  but the sequent calculus is convenient.
\end{remark}

\begin{remark}
  Punctuation.  In \(P;Q\vdash P\land Q\), the punctuation mark ``;''
  corresponds to the logical constant \(\land\), but is not itself a
  logical constant.  It would commonly be glossed as the ``and'' of
  ordinary English, but ``together with'' is probably better
  (following Restall).  The critical point is that this rule of
  \(\land\)-\textsc{intro}, we must not presuppose what the rule is
  intended to explain, namely logical \emph{and}.  So we cannot treat
  ``:'' as having that meaning.  On the other hand, a sequent is a
  formal object, so we need a meaning for ``;''.  The trick is to see
  that this rule serves as a meaning explanation for \emph{both} ``;''
  and \(\land\).  Restall puts it this way (for ``left conditional''):
  \(\to\) is the conditional \emph{for} the punctuation mark ``;''.
\end{remark}

\begin{remark}
  Alas, it's more complicated than that.  Restall does not use
  \(P;Q\vdash P\land Q\); instead he has \(X\vdash A\quad X\vdash
  B\models X\vdash A\land B\), written in vertical form with no
  punctuation between the premises.  That seems a little off; surely
  there must be an implicit punctuation mark.

  He says that an inference is a pair consisting of a set of
  consecutions (premises) and a single consecution (conclusion).  But
  that presupposes a notion of set, and leaves open the relation
  between members of that set.  It looks like a ``set of
  consecutions'' is just like a structure, only on a higher level.
  Structures (and thus their punctuation marks) only appear as
  antecedents within consecutions.

  The problem here is that in chapter 1 his ``consecution'' calculus
  is not a full Gentzen sequent calculus; that comes in chapter 6.
  Chapter 1 is basically a formulation of natural deduction using
  consecutions; drop the antecedents and \(\vdash\) and you get
  standard natural deduction.  The justification for using
  consecutions is that they make the assumptions explicit at every
  step - the subformula property.  (So structural rules are really
  about assumptions?)
\end{remark}

So what is the relation between the implicit punctuation mark between
premised consecutions and the explicit structural marks ``;'' and
``,'' in the antecedents of a consecution?

Consider the fusion introduction rule:

\begin{prooftree}
  \AxiomC{\(X\vdash A\)}
  \AxiomC{\(Y\vdash B\)}
  \RightLabel{\(\circ\)-\textsc{intro}}
  \BinaryInfC{\(X;Y\vdash X\circ Y\)}
\end{prooftree}

which he explains as ``[t]he connective \(\circ\) is a \emph{fusion}
for the punctuation mark `;'\ldots''.  But what is `;'?  Could we not
say with equal justification that `;' is the punctuation mark for
\(\circ\)?  Both of them together serve as the meaning explanation for
what we may do when we have \(X\vdash A\) ``together with'' \(Y\vdash
B\), as premises: gather the antecedents into a structure \(X;Y\), and
combine the succedents into a formula \(X\circ Y\).  So let's make
the implicit combinator in the premise explicit:

\begin{prooftree}
  \AxiomC{\(X\vdash A\)}
  \AxiomC{\(\odot\)}
  \AxiomC{\(Y\vdash B\)}
  \RightLabel{\(\circ\)-\textsc{intro}}
  \TrinaryInfC{\(X;Y\vdash X\circ Y\)}
\end{prooftree}

So we actually have three combinators, and this rule shows how they
are related, thus serving as a meaning explanation for \(\circ\) (and
for `;').  We might think of this rule as showing how the premises may
be dissassembled and then reassembled.  We can also think of this as
an elimination rule for \(\odot\) rather than an intro rule for
\(\circ\).

\begin{remark}
  Suggesting a more general principle: every intro rule may be
  construed as an elimination rule, and vice-versa.  Even in the case
  of Gentzen's original introduction rules, which contain an implicit
  premise combinator (\(\odot\)).  This is more obvious in the case of
  \emph{modus ponens}, where we have an implicit ``application''
  operator.  Using \(\odot\) just makes this explicit.  Modus ponens
  thus involves a double elimination: it removes both \(\odot\) and
  \(\to\).  Actually triple elimination, since it also removes \(A\).
  Construed as a (co-)introduction rule, modus ponens introduces `;'
  and \(B\) (since \(B\) is a subformula in the major premise).
\end{remark}

\begin{remark}
  Suggesting that the so-called intro and elim rules may be viewed as
  restructuring (as opposed to rearrangment) rules; that is, rules for
  taking apart (a gathering of) premises and restructuring the pieces
  to form a conclusion.  Extract raw materials from premises, build
  something new from them - but ``new'' only in the sense of
  ``different form''.  On this view it is the harmony/symmetry between
  the paired restructuring rules that matters, rather than the
  distinction between intro and elim.  In other words ``intro'' and
  ``elim'' are not primitive concepts.  The harmonious restructuring
  rules together provide meaning explanations, regardless of
  intro/elim concepts.
\end{remark}

We might also think of this as swapping: out with the old, in with the
new.  E.g. for \(\land\)-\textsc{intro} we swap out \(\odot\) for
\(\land\); for \(\land\)-\textsc{elim}, we swap out \(A\land B\) for
one of its components, \(A\) (or \(B\)).  For fusion intro, we swap
out \(\odot\) for `;` and \(\circ\).  Etc.

Another key point: the conclusion is not merely the same as the
premises, only rearranged.  It adds something new, i.e. new knowledge?
See Prawitz, Epistemic Significance of Valid Inference.

If we add the structural rules, then we can also add, subtract, and
rearrange components.

When we get to the full Gentzen sequent calculus things get more
complicated, since we can replace components by others that entail
them; e.g. for \(\land\textsf{L}_1\) we go from \(X(A)\vdash C\) to
\(X(A\land B)\vdash C\).  This is related to the fact that we have
introduced another var, \(C\), not directly involved in the meaning of
\(\land\).  Note that this sort of move amounts to going backwards in
a proof, inferring that since we have \(X(A)\) we must or may have had
\(X(A\land B)\).

So we can make three sorts of moves: restructuring of (sets of)
premise sequents, restructuring of sequent components (add, delete,
rearrange), and ``retreating'' (moving backwards).

\section{Inferential Typed Calculus}

\section{Calculi}

\subsection{Prawitz}

\subsubsection{Natural Deduction}

P. 20:

\medskip

%% AND
  \AxiomC{\(A\)}
  \AxiomC{\(B\)}
  \LeftLabel{\(\&\)I)}
  \BinaryInfC{\(A\& B\)}
\DisplayProof
\quad\quad
  \AxiomC{\(A\&B\)}
  \LeftLabel{\(\&\)E)}
  \UnaryInfC{\(A\)}
\DisplayProof
\quad
  \AxiomC{\(A\&B\)}
  \UnaryInfC{\(B\)}
\DisplayProof

\bigskip

%%  OR
\bottomAlignProof
  \LeftLabel{\(\lor\)I)}
  \AxiomC{\(A\)}
  \UnaryInfC{\(A\lor B\)}
\DisplayProof
\quad
\bottomAlignProof
  \AxiomC{\(B\)}
  \UnaryInfC{\(A\lor B\)}
\DisplayProof
\quad\quad
\bottomAlignProof
\AxiomC{\(A\lor B\)}
\AxiomC{(\(A\))}
\noLine
\UnaryInfC{\(C\)}
\AxiomC{(\(B\))}
\noLine
\UnaryInfC{\(C\)}
\LeftLabel{\(\lor\)E)}
\TrinaryInfC{\(C\)}
\DisplayProof

\bigskip
%%  Implication
\bottomAlignProof
  \AxiomC{\((A)\)}
  \noLine
  \UnaryInfC{\(B\)}
  \LeftLabel{\(\supset\)I)}
  \UnaryInfC{\(A\supset B\)}
\DisplayProof
\quad\quad
\bottomAlignProof
\AxiomC{\(A\)}
\AxiomC{\(A\to B\)}
\LeftLabel{\(\supset\)E)}
\BinaryInfC{\(B\)}
\DisplayProof

\bigskip
%%  Universal Quant
\bottomAlignProof
  \AxiomC{\(A\)}
  \LeftLabel{\(\forall\)I)}
  \UnaryInfC{\(\forall x A_x^a\)}
\DisplayProof
\quad\quad
\bottomAlignProof
\AxiomC{\(\forall x A\)}
\LeftLabel{\(\forall\)E)}
\UnaryInfC{\(A_t^x\)}
\DisplayProof

\bigskip
%%  Existential Quant
\bottomAlignProof
  \AxiomC{\(A_t^x\)}
  \LeftLabel{\(\exists\)I)}
  \UnaryInfC{\(\exists x A\)}
\DisplayProof
\quad\quad
\bottomAlignProof
\AxiomC{\(\exists x A\)}
\AxiomC{\((A_a^x)\)}
\noLine
\UnaryInfC{B}
\LeftLabel{\(\exists\)E)}
\BinaryInfC{\(B\)}
\DisplayProof

\bigskip
%%  False
\def\labelSpacing{12pt}
\bottomAlignProof
  \AxiomC{\rotatebox{90}{\(\succ\)}}
  \LeftLabel{\rotatebox{90}{\(\succ\)}\(_i\))}
  \UnaryInfC{\(A\)}
\DisplayProof
\quad\quad
\bottomAlignProof
\AxiomC{\((\sim A)\)}
\noLine
\UnaryInfC{\rotatebox{90}{\(\succ\)}}
\LeftLabel{\rotatebox{90}{\(\succ\)}\(_c\)}
\UnaryInfC{\(A\)}
\DisplayProof
\def\labelSpacing{3pt}

\subsubsection{Sequent Calculus}

P. 89:

\bigskip
%%  AND
\bottomAlignProof
  \AxiomC{\(\Gamma\to\Delta,A\)}
  \AxiomC{\(\Gamma\to\Delta,B\)}
  \LeftLabel{\(\to\&\))}
  \BinaryInfC{\(\Gamma\to\Delta,A\& B\)}
\DisplayProof
\quad\quad
\bottomAlignProof
\AxiomC{\(A,\Gamma\to\Delta\)}
\LeftLabel{\(\&\to\))}
\UnaryInfC{\(A\&B,\Gamma\to\Delta\)}
\DisplayProof
\quad
\bottomAlignProof
\AxiomC{\(B,\Gamma\to\Delta\)}
\UnaryInfC{\(A\&B,\Gamma\to\Delta\)}
\DisplayProof

\bigskip
%%  OR
\bottomAlignProof
  \AxiomC{\(\Gamma\to\Delta,A\)}
  \LeftLabel{\(\to\lor\))}
  \UnaryInfC{\(\Gamma\to\Delta,A\lor B\)}
\DisplayProof
\quad
\bottomAlignProof
  \AxiomC{\(\Gamma\to\Delta,B\)}
  \UnaryInfC{\(\Gamma\to\Delta,A\lor B\)}
\DisplayProof
\quad\quad
\bottomAlignProof
\AxiomC{\(A,\Gamma\to\Delta\)}
\AxiomC{\(B,\Gamma\to\Delta\)}
\LeftLabel{\(\lor\to\))}
\BinaryInfC{\(A\lor B,\Gamma\to\Delta\)}
\DisplayProof

\bigskip
%%  Implication
\bottomAlignProof
  \AxiomC{\(A,\Gamma\to\Delta,B\)}
  \LeftLabel{\(\to\supset\))}
  \UnaryInfC{\(\Gamma\to\Delta,A\supset B\)}
\DisplayProof
\quad\quad
\bottomAlignProof
\AxiomC{\(\Gamma\to\Delta_1,A\)}
\AxiomC{\(B,\Gamma\to\Delta_2\)}
\LeftLabel{\(\supset\to\))}
\BinaryInfC{\(A\supset B,\Gamma\to\Delta_1\cup\Delta_2\)}
\DisplayProof

\bigskip
%%  Universal Quant
\bottomAlignProof
  \AxiomC{\(\Gamma\to\Delta,A\)}
  \LeftLabel{\(\to\forall\))}
  \UnaryInfC{\(\Gamma\to\Delta,\forall x A_x^a\)}
\DisplayProof
\quad\quad
\bottomAlignProof
\AxiomC{\(A_t^x,\Gamma\to\Delta\)}
\LeftLabel{\(\forall\to\))}
\UnaryInfC{\(\forall x A,\Gamma\to\Delta\)}
\DisplayProof

\bigskip
%%  Existential Quant
\bottomAlignProof
  \AxiomC{\(\Gamma\to\Delta,A_t^x\)}
  \LeftLabel{\(\to\exists\))}
  \UnaryInfC{\(\Gamma\to\Delta,\exists x A\)}
\DisplayProof
\quad\quad
\bottomAlignProof
\AxiomC{\(A,\Gamma\to\Delta\)}
\LeftLabel{\(\exists\to\))}
\UnaryInfC{\(\exists x A_x^a,\Gamma\to\Delta\)}
\DisplayProof

\bigskip
%%  False
\def\labelSpacing{12pt}
\bottomAlignProof
  \AxiomC{\(\Gamma\to\Delta\),\ \rotatebox{90}{\(\succ\)}}
  \LeftLabel{\rotatebox{90}{\(\succ\)})}
  \UnaryInfC{\(\Gamma\to\Delta,A\)}
\DisplayProof
\def\labelSpacing{3pt}

\bigskip

Prawitz excludes \(\sim\) as a primitive.  To facilitate comparison with
natural deduction, omit \rotatebox{90}{\(\succ\)} and add:

\bigskip
%%  Neg
\bottomAlignProof
  \AxiomC{\(A,\Gamma\to\Delta\)}
  \LeftLabel{\(\to\sim\))}
  \UnaryInfC{\(\Gamma\to\Delta,\sim A\)}
\DisplayProof
\quad\quad
\bottomAlignProof
\AxiomC{\(\Gamma\to\Delta,A\)}
\LeftLabel{\(\sim\to\))}
\UnaryInfC{\(\sim A,\Gamma\to\Delta\)}
\DisplayProof

\bigskip
%%  Neg
\bottomAlignProof
  \AxiomC{\(\Gamma\to\)}
  \RightLabel{(\textit{thinning})}
  \UnaryInfC{\(\Gamma\to\ A\)}
\DisplayProof

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Takeuti}

P. 9-11

\subsubsection{Structural Rules}

\bigskip
1.1  \emph{Weakening}
\medskip

\begin{center}
\bottomAlignProof
  \AxiomC{\(\Gamma\to\Delta\)}
  \LeftLabel{left:}
  \UnaryInfC{\(D,\Gamma\to\Delta\)}
\DisplayProof
\quad\quad
\bottomAlignProof
\AxiomC{\(\Gamma\to\Delta\)}
\LeftLabel{right:}
\UnaryInfC{\(\Gamma\to\Delta,D\)}
\DisplayProof
\end{center}

\bigskip\noindent
1.2) \emph{Contraction}
\medskip

\begin{center}
\bottomAlignProof
  \AxiomC{\(D,D,\Gamma\to\Delta\)}
  \LeftLabel{left:}
  \UnaryInfC{\(D,\Gamma\to\Delta\)}
\DisplayProof
\quad\quad
\bottomAlignProof
\AxiomC{\(\Gamma\to\Delta,D,D\)}
\LeftLabel{right:}
\UnaryInfC{\(\Gamma\to\Delta,D\)}
\DisplayProof
\end{center}

\bigskip\noindent
1.3) \emph{Exchange}
\medskip

\begin{center}
\bottomAlignProof
  \AxiomC{\(\Gamma,C,D,\Pi\to\Delta\)}
  \LeftLabel{left:}
  \UnaryInfC{\(\Gamma,D,C,\Pi\to\Delta\)}
\DisplayProof
\quad\quad
\bottomAlignProof
\AxiomC{\(\Gamma\to\Delta,C,D,\Lambda\)}
\LeftLabel{right:}
\UnaryInfC{\(\Gamma\to\Delta,D,C,\Lambda\)}
\DisplayProof
\end{center}

\bigskip\noindent
1.4) \emph{Cut}
\medskip

\begin{center}
\bottomAlignProof
  \AxiomC{\(\Gamma\to\Delta,D\)}
  \AxiomC{\(D,\Pi\to\Lambda\)}
  \BinaryInfC{\(\Gamma,\Pi\to\Delta,\Lambda\)}
\DisplayProof
\end{center}

\subsubsection{Logical Rules}

\bigskip\noindent
2.1) \(\neg\)
\medskip


\bigskip\noindent
2.2) \(\land\)
\medskip

\bottomAlignProof
  \AxiomC{\(C,\Gamma\to\Delta\)}
  \LeftLabel{\(\land\)L:}
  \UnaryInfC{\(C\land D,\Gamma\to\Delta\)}
\DisplayProof
\quad
\bottomAlignProof
  \AxiomC{\(D,\Gamma\to\Delta\)}
  \UnaryInfC{\(C\land D,\Gamma\to\Delta\)}
  \DisplayProof
\quad\quad
\bottomAlignProof
\AxiomC{\(\Gamma\to\Delta,C\)}
\AxiomC{\(\Gamma\to\Delta,D\)}
\LeftLabel{\(\land\)R:}
\BinaryInfC{\(\Gamma\to\Delta,C\land D\)}
\DisplayProof

\bigskip\noindent
2.3) \(\lor\)
\medskip

\bottomAlignProof
  \AxiomC{\(C,\Gamma\to\Delta\)}
  \AxiomC{\(D,\Gamma\to\Delta\)}
  \LeftLabel{\(\lor\)L:}
  \BinaryInfC{\(C\lor D,\Gamma\to\Delta\)}
\DisplayProof
\quad\quad
\bottomAlignProof
\AxiomC{\(\Gamma\to\Delta,C\)}
\LeftLabel{\(\lor\)R:}
\UnaryInfC{\(\Gamma\to\Delta,C\lor D\)}
\DisplayProof
\quad
\bottomAlignProof
\AxiomC{\(\Gamma\to\Delta,D\)}
\UnaryInfC{\(\Gamma\to\Delta,C\lor D\)}
\DisplayProof

\bigskip\noindent
2.4) \(\supset\) %%  Implication
\medskip

\bottomAlignProof
  \AxiomC{\(\Gamma\to\Delta,C\)}
  \AxiomC{\(D,\Pi\to\Lambda\)}
  \LeftLabel{\(\supset\)L:}
  \BinaryInfC{\(C\supset D,\Gamma,\Pi\to\Delta,\Lambda\)}
\DisplayProof
\quad\quad
\bottomAlignProof
\AxiomC{\(C,\Gamma\to\Delta,D\)}
\LeftLabel{\(\supset\)R:}
\UnaryInfC{\(\Gamma\to\Delta,C\supset D\)}
\DisplayProof

\bigskip\noindent
2.5) \(\forall\) %%  Universal Quant
\medskip

\bottomAlignProof
  \AxiomC{\(F(t),\Gamma\to\Delta\)}
  \LeftLabel{\(\forall\)L:}
  \UnaryInfC{\(\forall x F(x),\Gamma\to\Delta\)}
\DisplayProof
\quad\quad
\bottomAlignProof
\AxiomC{\(\Gamma\to\Delta,F(a)\)}
\LeftLabel{\(\forall\):R:}
\UnaryInfC{\(\Gamma\to\Delta,\forall x F(x),\)}
\DisplayProof

\medskip
where \(t\) is an arbitrary term and \(a\) does not occur in the lower
sequent... The \(a\) in \(\forall\)R is called the
\emph{eigenvariable} of this inference.

\bigskip\noindent
2.6) \(\exists\)
\medskip

%%  Existential Quant
\bottomAlignProof
  \AxiomC{\(F(a),\Gamma\to\Delta\)}
  \LeftLabel{\(\exists\)L:}
  \UnaryInfC{\(\exists x F(x),\Gamma\to\Delta\)}
\DisplayProof
\quad\quad
\bottomAlignProof
\AxiomC{\(\Gamma\to\Delta,F(t)\)}
\LeftLabel{\(\exists\)R:}
\UnaryInfC{\(\Gamma\to\Delta,\exists x F(x),\)}
\DisplayProof

\medskip
where \(a\) does not occur in the lower sequent and \(t\) is an
arbitrary term.  The \(a\) in \(\exists\)L is called the
\emph{eigenvariable} of this inference.

\bigskip
%%  False
\def\labelSpacing{12pt}
\bottomAlignProof
  \AxiomC{\(\Gamma\to\Delta\),\ \rotatebox{90}{\(\succ\)}}
  \LeftLabel{\rotatebox{90}{\(\succ\)})}
  \UnaryInfC{\(\Gamma\to\Delta,A\)}
\DisplayProof
\def\labelSpacing{3pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Restall}

\end{document}
